Adjusted False Positive Rate,"The proportion of negative cases incorrectly classified as positive, adjusted for class imbalance or business context."
Agreement Rate,The proportion of cases where model predictions agree with actual outcomes.
Area Under Precision-Recall Curve,The area under the curve that plots precision versus recall at various thresholds.
Area Under the Curve,"The area under the ROC curve, measuring the ability of a model to distinguish between classes."
AUC Relative Decrease,The percentage decrease in AUC compared to a reference or baseline model.
Bad Case Rate,"The proportion of cases classified as ""bad"" or undesirable outcomes."
Capture Rate,The proportion of actual positives that are correctly identified by the model.
Classification Statistics,"Summary statistics for classification model performance, such as accuracy, precision, and recall."
Correct Acceptance Rate,The percentage of correctly accepted cases out of all cases.
Correct Detection Rate,The proportion of correctly detected cases out of all cases.
F1 Score,"The harmonic mean of precision and recall, balancing false positives and false negatives."
False Negative Percentage,The percentage of actual positives incorrectly predicted as negatives.
False Negative Rate,The proportion of actual positives incorrectly classified as negatives.
False Positive Rate,The proportion of actual negatives incorrectly classified as positives.
False Positive Ratio,The ratio of false positives to total predictions.
Gini Coefficient,A measure of the discriminatory power of a model; higher values indicate better separation.
Kolmogorov-Smirnov Statistic,"The maximum difference between the cumulative distributions of two samples, used to assess class separation."
KS Score,"The maximum difference between cumulative distributions of two samples, used to assess class separation."
Negative Predictive Value,The proportion of negative predictions that are actually negative.
Overprediction Rate,The proportion of cases where predictions exceed actual values.
Precision,The proportion of true positive predictions among all positive predictions made by the model.
Rate Difference,"The difference between two rates, such as approval rates or error rates."
Recall,The proportion of true positive predictions among all actual positive cases.
Receiver Operating Characteristic,A curve showing the trade-off between true positive rate and false positive rate.
Relative Bad Rate Difference,"The difference in bad rates (e.g., default rates) between two groups, normalized by a reference."
Total False Positive Rate,The overall rate of false positives across all predictions.
True Detection Rate,The proportion of actual positives correctly identified by the model.
True Positive Rate (Recall),The proportion of actual positives correctly classified as positives.
Underprediction Rate,The proportion of cases where predictions are below actual values.
Valid Detection Rate,The proportion of valid cases correctly detected by the model.
