Group,Sub-Group,Metric,Definition,Default,Custom Metric,Metric to Develop,claude
Model Performance,Classification and Discrimination,Adjusted False Positive Rate,"The proportion of negative cases incorrectly classified as positive, adjusted for class imbalance or business context.",Confusion matrix rates (binary/multi-class),binary-classification/positive-class-error-profile.md,,FEASIBLE - Can extend confusion matrix metrics with class imbalance adjustments. Custom metric referenced in positive-class-error-profile.md provides FPR calculation framework.
Model Performance,Classification and Discrimination,Agreement Rate,The proportion of cases where model predictions agree with actual outcomes.,,"binary-classification/detection-acceptance-profile.md, multi-classification/exact-match-ratio.md",,"FEASIBLE - Can compute as (TP+TN)/Total from confusion matrix. Examples: detection-acceptance-profile.md, exact-match-ratio.md provide agreement calculation patterns."
Model Performance,Classification and Discrimination,Area Under Precision-Recall Curve,The area under the curve that plots precision versus recall at various thresholds.,,,"Extend curve-based-discrimination.md to compute PR-AUC: calculate precision/recall pairs across thresholds, use trapezoidal rule for area","FEASIBLE - Extend curve-based-discrimination.md pattern: compute precision/recall pairs at multiple thresholds, apply trapezoidal integration (similar to ROC-AUC calculation)."
Model Performance,Classification and Discrimination,Area Under the Curve,The area under the ROC curve measuring the ability of a model to distinguish between classes.,,binary-classification/curve-based-discrimination.md,,FEASIBLE - Already implemented in curve-based-discrimination.md. Computes AUC-ROC using trapezoidal rule on TPR/FPR pairs.
Model Performance,Classification and Discrimination,AUC Relative Decrease,The percentage decrease in AUC compared to a reference or baseline model.,,,"Extend curve-based-discrimination.md: compare current AUC to reference dataset AUC, compute percentage decrease","FEASIBLE - Extend curve-based-discrimination.md: compute current AUC and compare to baseline/reference period AUC, calculate (baseline_AUC - current_AUC)/baseline_AUC × 100."
Model Performance,Classification and Discrimination,Bad Case Rate,"The proportion of cases classified as """"bad"""" or undesirable outcomes.",,binary-classification/positive-class-error-profile.md,,FEASIBLE - Already referenced in positive-class-error-profile.md. Can compute as count of predicted positives that are actual negatives divided by total cases.
Model Performance,Classification and Discrimination,Capture Rate,The proportion of actual positives that are correctly identified by the model.,,binary-classification/detection-acceptance-profile.md,,FEASIBLE - Already implemented in detection-acceptance-profile.md. Computes as (TP+FP)/Total - proportion of population captured as positive.
Model Performance,Classification and Discrimination,Classification Statistics,"Summary statistics for classification model performance, such as accuracy, precision, and recall.",Accuracy precision recall F1 score (binary/multi-class),"binary-classification/detection-acceptance-profile.md, binary-classification/positive-class-error-profile.md, multi-classification/multi-label-precision-recall-f1.md, multi-classification/multi-label-confusion-matrix.md",,"SUPPORTED - Platform default metrics include accuracy, precision, recall, and F1 score for binary/multi-class classification. Multiple examples provided."
Model Performance,Classification and Discrimination,Correct Acceptance Rate,The percentage of correctly accepted cases out of all cases.,,binary-classification/detection-acceptance-profile.md,,FEASIBLE - Already implemented in detection-acceptance-profile.md. Computes as TN/Total - fraction of all cases correctly accepted.
Model Performance,Classification and Discrimination,Correct Detection Rate,The proportion of correctly detected cases out of all cases.,,binary-classification/detection-acceptance-profile.md,,FEASIBLE - Already implemented in detection-acceptance-profile.md. Computes as (TP+TN)/Total - overall accuracy.
Model Performance,Classification and Discrimination,F1 Score,The harmonic mean of precision and recall balancing false positives and false negatives.,F1 score (binary/multi-class),multi-classification/multi-label-precision-recall-f1.md,,SUPPORTED - Platform default metric for binary/multi-class. Also available as custom metric in multi-label-precision-recall-f1.md.
Model Performance,Classification and Discrimination,False Negative Percentage,The percentage of actual positives incorrectly predicted as negatives.,Confusion matrix rates (binary/multi-class),multi-classification/multi-label-confusion-matrix.md,,SUPPORTED - Platform default confusion matrix rates include FN%. Custom implementation in multi-label-confusion-matrix.md.
Model Performance,Classification and Discrimination,False Negative Rate,The proportion of actual positives incorrectly classified as negatives.,Confusion matrix rates (binary/multi-class),multi-classification/multi-label-confusion-matrix.md,,SUPPORTED - Platform default confusion matrix rates include FNR. Custom implementation in multi-label-confusion-matrix.md.
Model Performance,Classification and Discrimination,False Positive Rate,The proportion of actual negatives incorrectly classified as positives.,Confusion matrix rates (binary/multi-class),"binary-classification/positive-class-error-profile.md, multi-classification/multi-label-confusion-matrix.md",,SUPPORTED - Platform default confusion matrix rates include FPR. Custom implementations in positive-class-error-profile.md and multi-label-confusion-matrix.md.
Model Performance,Classification and Discrimination,False Positive Ratio,The ratio of false positives to total predictions.,Confusion matrix rates (binary/multi-class),binary-classification/positive-class-error-profile.md,,FEASIBLE - Already referenced in positive-class-error-profile.md. Computes as FP/(TP+FP) or FP/Total depending on definition.
Model Performance,Classification and Discrimination,Gini Coefficient,A measure of the discriminatory power of a model; higher values indicate better separation.,,"binary-classification/gini-coefficient.md, binary-classification/curve-based-discrimination.md",,FEASIBLE - Already implemented in gini-coefficient.md and curve-based-discrimination.md. Computes as 2×AUC-1.
Model Performance,Classification and Discrimination,Kolmogorov-Smirnov Statistic,The maximum difference between the cumulative distributions of two samples used to assess class separation.,,binary-classification/curve-based-discrimination.md,,FEASIBLE - Already implemented in curve-based-discrimination.md. Computes max|TPR-FPR| across all thresholds.
Model Performance,Classification and Discrimination,KS Score,The maximum difference between cumulative distributions of two samples used to assess class separation.,,binary-classification/curve-based-discrimination.md,,FEASIBLE - Already implemented in curve-based-discrimination.md. Returns threshold value where KS statistic is maximized.
Model Performance,Classification and Discrimination,Negative Predictive Value,The proportion of negative predictions that are actually negative.,Confusion matrix rates (binary/multi-class),,"Extend multi-label-confusion-matrix.md or create new metric: NPV = TN / (TN + FN), compute from confusion matrix components",FEASIBLE - Extend confusion matrix patterns: NPV = TN/(TN+FN). Can use multi-label-confusion-matrix.md as base and add NPV calculation.
Model Performance,Classification and Discrimination,Overprediction Rate,The proportion of cases where predictions exceed actual values.,,binary-classification/positive-class-error-profile.md,,FEASIBLE - Already referenced in positive-class-error-profile.md. For regression: count where prediction > actual. For classification: FP rate.
Model Performance,Classification and Discrimination,Precision,The proportion of true positive predictions among all positive predictions made by the model.,Precision (binary/multi-class),"binary-classification/detection-acceptance-profile.md, multi-classification/multi-label-precision-recall-f1.md",,SUPPORTED - Platform default metric for binary/multi-class. Custom implementations in detection-acceptance-profile.md and multi-label-precision-recall-f1.md.
Model Performance,Classification and Discrimination,Rate Difference,The difference between two rates such as approval rates or error rates.,,binary-classification/subgroup-rate-comparison.md,,FEASIBLE - Already implemented in subgroup-rate-comparison.md. Computes difference between rates for different groups/segments.
Model Performance,Classification and Discrimination,Recall,The proportion of true positive predictions among all actual positive cases.,Recall (binary/multi-class),"binary-classification/detection-acceptance-profile.md, multi-classification/multi-label-precision-recall-f1.md",,SUPPORTED - Platform default metric for binary/multi-class. Custom implementations in detection-acceptance-profile.md and multi-label-precision-recall-f1.md.
Model Performance,Classification and Discrimination,Receiver Operating Characteristic,A curve showing the trade-off between true positive rate and false positive rate.,,binary-classification/curve-based-discrimination.md,,"FEASIBLE - Already implemented in curve-based-discrimination.md. Generates ROC curve points (FPR, TPR) across thresholds."
Model Performance,Classification and Discrimination,Relative Bad Rate Difference,The difference in bad rates (e.g. default rates) between two groups normalized by a reference.,,binary-classification/subgroup-rate-comparison.md,,FEASIBLE - Already implemented in subgroup-rate-comparison.md. Computes (rate_group1 - rate_group2)/rate_reference for normalized comparison.
Model Performance,Classification and Discrimination,Total False Positive Rate,The overall rate of false positives across all predictions.,Confusion matrix rates (binary/multi-class),binary-classification/positive-class-error-profile.md,,SUPPORTED - Platform default confusion matrix rates include FPR. Custom implementation in positive-class-error-profile.md.
Model Performance,Classification and Discrimination,True Detection Rate,The proportion of actual positives correctly identified by the model.,,binary-classification/detection-acceptance-profile.md,,FEASIBLE - Already implemented in detection-acceptance-profile.md. Computes as TP/(TP+FN) - same as recall/TPR.
Model Performance,Classification and Discrimination,True Positive Rate (Recall),The proportion of actual positives correctly classified as positives.,Recall (binary/multi-class),binary-classification/detection-acceptance-profile.md,,SUPPORTED - Platform default recall metric. Custom implementation in detection-acceptance-profile.md.
Model Performance,Classification and Discrimination,Underprediction Rate,The proportion of cases where predictions are below actual values.,,binary-classification/positive-class-error-profile.md,,FEASIBLE - Already referenced in positive-class-error-profile.md. For regression: count where prediction < actual. For classification: FN rate.
Model Performance,Classification and Discrimination,Valid Detection Rate,The proportion of valid cases correctly detected by the model.,,"binary-classification/detection-acceptance-profile.md, binary-classification/positive-class-error-profile.md",,FEASIBLE - Already implemented in detection-acceptance-profile.md and positive-class-error-profile.md. Computes as (TP+TN)/Total - accuracy.
Model Performance,Clustering & Association,Kendall's Tau,A statistic that measures ordinal association between two measured quantities.,,binary-classification/rank-association-profile.md,,FEASIBLE - Already implemented in rank-association-profile.md. Computes ordinal association using SQL with ranking functions.
Model Performance,Clustering & Association,Silhouette Score,A measure of clustering quality by evaluating how similar an object is to its own cluster compared to other clusters.,,,"Create custom metric: require cluster_assignment column (categorical) and feature columns (multiple float columns or float array) in inference dataset. Use self-join pattern (similar to label co-occurrence) to create all point pairs. Compute Euclidean distance: SQRT(SUM(POWER(feature1[i] - feature2[i], 2))) using array unnest with WITH ORDINALITY or multiple float columns. For each point i: a_i = AVG(distance) WHERE cluster_i = cluster_j AND i != j, b_i = MIN(AVG(distance)) WHERE cluster_i != cluster_j GROUP BY cluster_j, silhouette_i = (b_i - a_i) / GREATEST(a_i, b_i). Aggregate AVG(silhouette_i) per time bucket. SQL capabilities confirmed: self-joins (Pattern 1), SQRT/POWER functions (RMSE example), array operations with unnest/CROSS JOIN LATERAL, aggregations. Note: O(n²) complexity - computationally expensive for large datasets, recommend time-bucketing and consider sampling for very large datasets.","COMPLEX BUT FEASIBLE - Requires O(n²) distance calculations via self-joins. SQL supports needed operations (SQRT, POWER, self-joins, aggregations). Consider sampling for large datasets. Detailed implementation guidance provided in 'Metric to Develop' column."
Model Performance,Clustering & Association,Spearman's Rank Correlation Coefficient,A statistic that measures the strength and direction of association between two ranked variables.,,binary-classification/rank-association-profile.md,,FEASIBLE - Already implemented in rank-association-profile.md. Computes rank correlation using SQL ranking functions.
Model Performance,Data Quality & Completeness,Data Completeness Percentage,The percentage of data records that are complete and contain all required fields.,,,Create custom metric: COUNT(complete_rows) / COUNT(all_rows) where complete_rows have all required columns non-NULL,FEASIBLE - Simple SQL: COUNT(CASE WHEN col1 IS NOT NULL AND col2 IS NOT NULL ... THEN 1 END)/COUNT(*) for required fields.
Model Performance,Extreme Value & Outlier,Extreme Overvaluation Rate,The proportion of cases where predictions are significantly higher than actual values.,,,"Extend regression/error-distribution.md or create new: define threshold (e.g. prediction > 2× actual), compute rate","FEASIBLE - Extend error-distribution.md: define threshold (e.g., prediction > 2×actual or prediction-actual > threshold), compute COUNT(condition)/COUNT(*) as rate."
Model Performance,Extreme Value & Outlier,Extreme Undervaluation Rate,The proportion of cases where predictions are significantly lower than actual values.,,,"Extend regression/error-distribution.md or create new: define threshold (e.g. prediction < 0.5× actual), compute rate","FEASIBLE - Extend error-distribution.md: define threshold (e.g., prediction < 0.5×actual or actual-prediction > threshold), compute COUNT(condition)/COUNT(*) as rate."
Model Performance,Model Stability & Drift,Drift Score,A quantitative measure of how much the distribution of input data or model predictions has changed over time compared to a reference period.,,,"Extend population-stability-index.md pattern: create generic drift metric comparing current vs reference distribution using KL divergence, Wasserstein distance, or similar","FEASIBLE - Extend population-stability-index.md pattern: compare current vs reference distributions using KL divergence, Wasserstein distance, or PSI-like binning approach."
Model Performance,Model Stability & Drift,Population Stability Index,A measure of changes in the distribution of a variable between two samples (e.g. time periods).,PSI (regression binary/multi-class),binary-classification/population-stability-index.md,,"SUPPORTED - Platform default metric PSI for regression, binary, and multi-class. Custom implementation in population-stability-index.md."
Model Performance,Prediction Accuracy & Error,Absolute Error,The absolute difference between a predicted value and the actual value.,,regression/error-distribution.md,,"FEASIBLE - Already implemented in error-distribution.md. Computes ABS(prediction - actual) per inference, stored as sketch metric."
Model Performance,Prediction Accuracy & Error,Accuracy,The proportion of correct predictions (true positives and true negatives) among all predictions.,Accuracy (binary/multi-class),"binary-classification/detection-acceptance-profile.md, binary-classification/positive-class-error-profile.md, multi-classification/exact-match-ratio.md",,"SUPPORTED - Platform default metric for binary/multi-class. Custom implementations in detection-acceptance-profile.md, positive-class-error-profile.md, exact-match-ratio.md."
Model Performance,Prediction Accuracy & Error,Core Accuracy at PPE 10% Threshold,The accuracy of predictions within a 10% error threshold.,,,"Create custom metric: COUNT(ABS(pred - actual) / actual <= 0.10) / COUNT(*) for regression, or extend percentage-error-metrics.md",FEASIBLE - Extend percentage-error-metrics.md: COUNT(ABS((pred-actual)/actual) <= 0.10)/COUNT(*). Filter to cases within 10% error threshold.
Model Performance,Prediction Accuracy & Error,Correct Prediction Accuracy,The percentage of correct predictions out of all predictions.,Accuracy (binary/multi-class),"binary-classification/detection-acceptance-profile.md, binary-classification/positive-class-error-profile.md",,SUPPORTED - Same as accuracy. Platform default metric for binary/multi-class.
Model Performance,Prediction Accuracy & Error,Cumulative Error,The total sum of errors across all predictions.,,regression/cumulative-error.md,,FEASIBLE - Already implemented in cumulative-error.md. Computes SUM(prediction - actual) aggregated over time buckets.
Model Performance,Prediction Accuracy & Error,Deviation Ratio,The ratio of the deviation of predictions from actual values.,,regression/percentage-error-metrics.md,,FEASIBLE - Already referenced in percentage-error-metrics.md. Computes ratio of error magnitude to actual value or prediction.
Model Performance,Prediction Accuracy & Error,End of term absolute error,The absolute difference between predicted and actual values at the end of a specified period.,,regression/end-of-term-absolute-error.md,,"FEASIBLE - Already implemented in end-of-term-absolute-error.md. Computes ABS(prediction-actual) at specific time points (e.g., end of period)."
Model Performance,Prediction Accuracy & Error,Forecast Error,The difference between predicted and actual values in forecasting.,,regression/error-distribution.md,,"FEASIBLE - Already implemented in error-distribution.md. Computes (prediction - actual) as signed error, stored as sketch metric."
Model Performance,Prediction Accuracy & Error,Log Loss,A metric that measures the uncertainty of predictions based on their probability estimates; lower values indicate better calibrated models.,,,"Create custom metric: -AVG(actual × LN(pred) + (1-actual) × LN(1-pred)) for binary classification, requires probability predictions",FEASIBLE - Use LN function: -AVG(actual×LN(pred)+(1-actual)×LN(1-pred)) for binary classification. Requires probability predictions (0-1 range). Add bounds checking for numerical stability.
Model Performance,Prediction Accuracy & Error,Maximum Error,The largest absolute error among all predictions.,,regression/maximum-errors.md,,FEASIBLE - Already implemented in maximum-errors.md. Uses sketch metric to track MAX(ABS(prediction - actual)) over time buckets.
Model Performance,Prediction Accuracy & Error,Maximum Relative Error,The largest relative error among all predictions.,,regression/maximum-errors.md,,FEASIBLE - Already implemented in maximum-errors.md. Uses sketch metric to track MAX(ABS((prediction-actual)/actual)) over time buckets.
Model Performance,Prediction Accuracy & Error,Mean Absolute Deviation,The average of the absolute differences between each value and the mean of the dataset.,,regression/mean-absolute-deviation.md,,FEASIBLE - Already implemented in mean-absolute-deviation.md. Computes AVG(ABS(value - mean)) for distribution spread analysis.
Model Performance,Prediction Accuracy & Error,Mean Absolute Error,The average of absolute errors between predicted and actual values.,MAE (regression),regression/error-distribution.md,,SUPPORTED - Platform default MAE metric for regression. Custom implementation in error-distribution.md.
Model Performance,Prediction Accuracy & Error,Mean Absolute Percentage Error,The average of absolute percentage errors between predicted and actual values.,,regression/percentage-error-metrics.md,,FEASIBLE - Already implemented in percentage-error-metrics.md. Computes AVG(ABS((prediction-actual)/actual)×100).
Model Performance,Prediction Accuracy & Error,Mean Squared Error,The average of squared differences between predicted and actual values.,MSE (regression),regression/root-mean-squared-error.md (RMSE² = MSE),,SUPPORTED - Platform default MSE metric for regression. Can compute from RMSE: MSE = RMSE².
Model Performance,Prediction Accuracy & Error,Median Absolute Percentage Error,The median of absolute percentage errors between predicted and actual values.,,regression/percentage-error-metrics.md,,FEASIBLE - Already implemented in percentage-error-metrics.md. Computes MEDIAN(ABS((prediction-actual)/actual)×100) using sketch quantile functions.
Model Performance,Prediction Accuracy & Error,Normalized Mean Absolute Deviation,The mean absolute deviation normalized by a reference value often the mean or median.,,regression/normalized-mean-absolute-deviation.md,,FEASIBLE - Already implemented in normalized-mean-absolute-deviation.md. Computes MAD/reference_value for scale-independent comparison.
Model Performance,Prediction Accuracy & Error,Percentage Accuracy,The proportion of correct predictions expressed as a percentage.,,,"Extend detection-acceptance-profile.md or create: (TP + TN) / Total × 100, similar to accuracy but expressed as percentage",FEASIBLE - Simple transformation: accuracy×100. Can extend detection-acceptance-profile.md or compute as ((TP+TN)/Total)×100.
Model Performance,Prediction Accuracy & Error,Root Mean Squared Error,The square root of the average squared differences between predicted and actual values.,RMSE (regression),regression/root-mean-squared-error.md,,SUPPORTED - Platform default RMSE metric for regression. Custom implementation in root-mean-squared-error.md.
Model Performance,Prediction Accuracy & Error,Trailing Twelve Months Correct Prediction Error,The error in predictions measured over the trailing twelve months.,,regression/trailing-twelve-months-error.md,,FEASIBLE - Already implemented in trailing-twelve-months-error.md. Aggregates errors over 12-month rolling window using time-based filtering.
Model Performance,Prediction Accuracy & Error,Weighted Mean Absolute Percentage Error,The mean absolute percentage error weighted by the importance or frequency of each observation.,,regression/weighted-mean-absolute-percentage-error.md,,FEASIBLE - Already implemented in weighted-mean-absolute-percentage-error.md. Computes weighted MAPE using weight column.
Model Performance,LLM Output Quality,Chunk Adherence,The degree to which a model's output correctly references or uses specific data chunks provided as context.,,,"Not supported (requires semantic analysis of text content and chunk references, needs external evaluation service)",NOT SUPPORTED - Requires semantic analysis of text content and chunk references. Arthur Platform SQL capabilities cannot evaluate whether output correctly references specific chunks. Needs external LLM-as-judge or specialized evaluation service.
Model Performance,LLM Output Quality,Chunk Utilization,The extent to which a model's output incorporates and leverages provided data chunks or context.,,,"Not supported (requires semantic analysis of text content and chunk references, needs external evaluation service)",NOT SUPPORTED - Requires semantic analysis of text content and chunk usage. Arthur Platform cannot assess whether chunks were meaningfully incorporated vs merely mentioned. Needs external LLM-as-judge or specialized evaluation service.
Model Performance,LLM Output Quality,Completeness,The degree to which a model's output fully addresses all aspects of the input prompt or task.,,,"Not supported (requires semantic understanding of prompt requirements vs output, needs external evaluation service)",NOT SUPPORTED - Requires semantic understanding of prompt requirements vs output completeness. Arthur Platform SQL capabilities cannot evaluate whether all aspects of a task were addressed. Needs external LLM-as-judge evaluation service.
Model Performance,LLM Output Quality,Instruction Adherence,The extent to which a model's output follows the explicit instructions in the input prompt.,,,"Not supported (requires semantic analysis of instructions vs output, needs external evaluation service)",NOT SUPPORTED - Requires semantic analysis of instructions vs output adherence. Arthur Platform cannot parse instructions and evaluate whether output follows them. Needs external LLM-as-judge or specialized evaluation service.
Model Performance,LLM Output Quality,Sentiment,The emotional tone (positive negative neutral) of text data typically from NLP models.,,,"Create custom metric if sentiment scores are pre-computed: aggregate sentiment scores (numeric) from external NLP service, or use if model outputs sentiment directly","CONDITIONAL - FEASIBLE if sentiment scores are pre-computed by external NLP service and stored in dataset as numeric values. NOT SUPPORTED if requires real-time sentiment analysis of text - Arthur Platform cannot perform NLP text analysis. Can aggregate pre-computed sentiment scores: AVG(sentiment_score), distribution analysis."
Model Performance,LLM Output Quality,Token Size,The number of tokens (words or subwords) processed or generated by a language model for an input or output.,Prompt/response token count (GenAI),,,SUPPORTED - Platform default metric for GenAI models. Tracks prompt/response token counts automatically.
Model Performance,Regression & Fit,R-Squared (Coefficient of Determination),The proportion of variance in the dependent variable explained by the model.,,,"Create custom metric: 1 - (SS_res / SS_tot) where SS_res = SUM((actual - pred)²), SS_tot = SUM((actual - mean(actual))²)","FEASIBLE - Create custom metric: 1 - (SS_res/SS_tot) where SS_res=SUM((actual-pred)²) and SS_tot=SUM((actual-mean(actual))²). Requires computing mean in CTE, then residuals and total sum of squares."
Operational,Volume & Throughput,Average Daily Rate,The average rate per day over a specified period.,,,"Create custom metric: aggregate count metric divided by number of days, or use window function AVG() OVER time buckets",FEASIBLE - Simple aggregation: use COUNT/time window or AVG() OVER time buckets. Example: SUM(value)/COUNT(DISTINCT DATE(timestamp)) for daily rate.
Operational,Volume & Throughput,Click-Through Rate,The proportion of users who click on a specific link or advertisement.,,,"Create custom metric: COUNT(clicks) / COUNT(impressions) if click events are tracked in dataset, requires click/impression columns",CONDITIONAL - FEASIBLE if click/impression events tracked in dataset as separate rows or columns. Compute COUNT(clicks)/COUNT(impressions) or SUM(click_flag)/COUNT(*). NOT FEASIBLE without proper event tracking columns.
Operational,Volume & Throughput,Contact Rate,The proportion of cases where contact was successfully made.,,,Create custom metric: COUNT(contacted) / COUNT(total_cases) if contact status is tracked in dataset,CONDITIONAL - FEASIBLE if contact status tracked in dataset. Compute COUNT(contacted=true)/COUNT(*) or similar based on status column. NOT FEASIBLE without contact tracking data.
Operational,Volume & Throughput,Days to Closing,The number of days taken to close a case or transaction.,,,"Create custom metric: AVG(close_date - open_date) if both dates are in dataset, compute time difference",CONDITIONAL - FEASIBLE if both open_date and close_date columns exist in dataset. Compute AVG(close_date - open_date) or EXTRACT(DAY FROM close_date-open_date). NOT FEASIBLE without date columns.
Operational,Volume & Throughput,Lift in Average Daily Balance,The increase in average daily balance attributed to the model compared to a baseline.,,,"Create custom metric: compare AVG(balance) for model group vs baseline group, compute difference, requires baseline dataset or column",CONDITIONAL - FEASIBLE if balance data and baseline/control group identifiers exist. Compute AVG(balance WHERE model_group) - AVG(balance WHERE baseline_group). Requires proper experimental design columns. NOT FEASIBLE without baseline comparison data.
Operational,Volume & Throughput,Lift in Revenue,The increase in revenue attributed to the model compared to a baseline.,,,"Create custom metric: compare SUM(revenue) for model group vs baseline group, compute difference, requires baseline dataset or column",CONDITIONAL - FEASIBLE if revenue data and baseline/control group identifiers exist. Compute SUM(revenue WHERE model_group) - SUM(revenue WHERE baseline_group). Requires proper experimental design columns. NOT FEASIBLE without baseline comparison data.
Operational,Volume & Throughput,Lift in Settled Sales,The increase in settled sales attributed to the model compared to a baseline.,,,"Create custom metric: compare COUNT(settled_sales) for model group vs baseline group, compute difference, requires baseline dataset or column",CONDITIONAL - FEASIBLE if settled_sales data and baseline/control group identifiers exist. Compute COUNT(settled_sales WHERE model_group) - COUNT(settled_sales WHERE baseline_group). Requires proper experimental design columns. NOT FEASIBLE without baseline comparison data.
Operational,Volume & Throughput,Points to Double Odds,The number of scorecard points required to double the odds of a positive outcome.,,,"Create custom metric: analyze score distribution vs outcome, find score difference where odds ratio = 2, requires score and outcome columns",COMPLEX BUT FEASIBLE - Requires analyzing score distribution vs outcome to find score difference where odds ratio=2. Use window functions to compute cumulative odds at each score level: odds(s)=P(Y=1|score=s)/P(Y=0|score=s). Find Δs where odds(s+Δs)/odds(s)=2. Computationally intensive but achievable with CTEs and window functions.
Operational,Volume & Throughput,Score Utilization Percentage,The percentage of times a score is used in decision-making.,,,Create custom metric: COUNT(decisions_with_score) / COUNT(all_decisions) if decision tracking columns exist,"CONDITIONAL - FEASIBLE if decision tracking columns exist (e.g., score_used flag or decision_method column). Compute COUNT(score_used=true)/COUNT(all_decisions). NOT FEASIBLE without decision tracking metadata."
Operational,Volume & Throughput,Volume (General),The total number of data points transactions or requests processed within a specified time frame.,Inference count (all),multi-classification/multi-label-prediction-volume.md,,SUPPORTED - Platform default inference count metric for all model types. Custom implementation in multi-label-prediction-volume.md.
Operational,Volume & Throughput,Volume (LLM),The total number of LLM requests or responses processed within a given period.,Inference count (GenAI),multi-classification/multi-label-prediction-volume.md,,SUPPORTED - Platform default inference count metric for GenAI models. Custom implementation in multi-label-prediction-volume.md.
Operational,Volume & Throughput,API call volumes,The total number of API requests made within a specified time frame.,Inference count (all),multi-classification/multi-label-prediction-volume.md,,SUPPORTED - Same as inference count. Platform default metric for all model types.
Operational,Efficiency,Cost,The computational or monetary expense associated with processing a request or generating a response.,,,"Create custom metric: SUM(cost) if cost column exists in dataset, or aggregate cost from external billing data if available",CONDITIONAL - FEASIBLE if cost column exists in inference dataset. Compute SUM(cost) or AVG(cost) over time buckets. NOT FEASIBLE if cost data comes from external billing system not joined to inference data.
Operational,Efficiency,API latency,The time taken for an API to process a request and return a response typically measured in milliseconds.,,,"Create custom metric: AVG(response_time - request_time) if timestamps are tracked, use sketch metric for distribution","CONDITIONAL - FEASIBLE if request/response timestamps tracked in dataset. Compute response_time - request_time or EXTRACT(MILLISECONDS FROM response_timestamp-request_timestamp). Use sketch metric for distribution analysis (P50, P95, P99). NOT FEASIBLE without timestamp tracking."
Operational,Efficiency,Latency,The time taken for a model or API to process an input and return a result usually measured in milliseconds.,Latency (GenAI),,,"SUPPORTED - Platform default latency metric for GenAI models, tracked automatically."
Operational,Reliability,User Complaints,The number or rate of complaints received from users regarding model outputs.,,,"Create custom metric: COUNT(complaints) if complaint events are tracked in dataset, or aggregate from external complaint system if available","CONDITIONAL - FEASIBLE if complaint events tracked in dataset (e.g., complaint_flag or complaint_count column). Compute COUNT(complaints) or SUM(complaint_flag). NOT FEASIBLE if complaints tracked in external system not joined to inference data."
Operational,Reliability,API error rates,The proportion of API requests resulting in errors relative to the total number of requests.,,,"Create custom metric: COUNT(error_status) / COUNT(all_requests) if error status is tracked, requires error/status column","CONDITIONAL - FEASIBLE if error status tracked in dataset (e.g., status_code or error_flag column). Compute COUNT(error=true)/COUNT(*) or COUNT(status≥400)/COUNT(*). NOT FEASIBLE without error status tracking."
Operational,Reliability,HTTP Errors,The count or rate of HTTP error responses (e.g. 4xx 5xx) indicating failed or problematic API requests.,,,Create custom metric: COUNT(HTTP_status >= 400) / COUNT(all_requests) if HTTP status codes are tracked in dataset,CONDITIONAL - FEASIBLE if HTTP status codes tracked in dataset. Compute COUNT(http_status>=400)/COUNT(*) to get error rate. Can segment by error type (4xx client errors vs 5xx server errors). NOT FEASIBLE without HTTP status code tracking.
